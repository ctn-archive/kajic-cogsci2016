# Towards Biologically Plausible Representation of Word Associations

## Reproducing plots and figures

### Requirements

Most of the requirements should be installable with `pip`.

* [Python 2.7](https://www.python.org/) (Python 3 might work, but is untested)
* [NumPy](http://www.numpy.org/)
* [SciPy](http://www.scipy.org/)
* [matplotlib](http://matplotlib.org/)
* [pandas](http://pandas.pydata.org/)
* [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)
* [statsmodels](http://statsmodels.sourceforge.net/)
* [doit](http://pydoit.org/)

#### For running the spiking neural network model

* [Nengo](https://github.com/nengo/nengo)
* [ctn_benchmarks](https://github.com/ctn-waterloo/ctn_benchmarks)

To do the batch run to get the accuracy data for Figure 5:

* [psyrun](https://github.com/jgosmann/psyrun)


### Fetching and processing data

Run `doit` to fetch and process most of the required data.

To obtain the data for the spiking neural network plotted in Figure 4 run
`python sparat/model/benchmark.py`. Note that this requires up to 6GB of memory.

To obtain the data for the accuracy of the spiking neural network plotted in
Figure 5 run `psy-doit`. Note that this requires up to 6GB of memory and can
take a long time.


### Creating tables and plots

After you performed the previous step to generate the required data, you find
Jupyter notebooks in `notebooks`. Run these to recreate the plots and tables in
the paper.


## Repository organization

### data
Data files not included in the directory and are either downloaded from
external resources or generated by scripts.

### figures
Figures used in the paper.

### notebooks
Jupyter notebooks with data analyses and plotting.

### psy-tasks
Task definition files for the serial farming tool `psyrun`.

### scripts
Data processing scripts meant to be invoked from the command line.

### sparat
Python source code for processing the data and the model excluding command line
tools.

### tables
Generated LaTeX tables for the paper.

### txt
Documentation including the CogSci paper.
